# Series de tiempo - Detecci√≥n de Agua

#Este proyecto se va a realizar en Kaggle.
#El prop√≥sito es predecir el volumen de agua
#Acea Group es la mayor empresa de agua en Italia y busca predecir los niveles diarios de agua en fuentes, 
# r√≠os y acu√≠feros para gestionar mejor el consumo y proteger estos recursos, especialmente porque se llenan en invierno
#  y se vac√≠an en verano.
#Los datos representan distintos tipos de cuerpos de agua, cada uno con variables propias porque su comportamiento es √∫nico. 
# Acea trabaja con manantiales, lagos, r√≠os y acu√≠feros, y necesita entender su disponibilidad para proteger el recurso.
# El reto es predecir cu√°nta agua habr√° en cada tipo, 
# creando cuatro modelos ‚Äîuno por categor√≠a‚Äî que permitan anticipar niveles diarios o mensuales y asegurar una buena
#  planificaci√≥n del uso del agua.

#La evaluaci√≥n se basa en tres √°reas: metodolog√≠a, presentaci√≥n y aplicaci√≥n, cada una con un m√°ximo de 5 puntos. 
# Se revisa si los modelos son apropiados, si se mide su desempe√±o (MAE y RMSE), 
# si el notebook explica bien la historia con visualizaciones y an√°lisis claros, 
# y si el modelo puede predecir niveles o flujos de agua y aplicarse a nuevos conjuntos de datos.
# Las entregas deben enviarse al organizador y ser√°n evaluadas por Acea. 
# La competencia inici√≥ el 10 de diciembre de 2020, cerr√≥ el 17 de febrero de 2021 
# y los ganadores se anunciaron el 10 de marzo de 2021.

#El concurso utiliza nueve datasets totalmente independientes, cada uno representando un tipo distinto de cuerpo de agua. 
# Acea trabaja con cuatro categor√≠as: acu√≠feros (4 datasets), manantiales (3), un r√≠o y un lago. 
# Cada uno tiene variables propias seg√∫n su origen, alimentaci√≥n y comportamiento, 
# por lo que las caracter√≠sticas cambian entre un manantial, un lago, un acu√≠fero o un r√≠o.
# Los acu√≠feros incluyen Auser, Petrignano, Doganella y Luco, 
# cada uno influido por factores como lluvia, temperaturas, profundidad del agua o vol√∫menes drenados.
#  Los manantiales (Amiata, Madonna di Canneto y Lupa) se alimentan por infiltraci√≥n o cuencas espec√≠ficas.
#  El r√≠o Arno se eval√∫a por su nivel hidrom√©trico, y el lago Bilancino sirve como reserva para alimentar al Arno en verano. 
# Cada dataset exige predecir una variable clave distinta seg√∫n el comportamiento del cuerpo de agua.

# luego del EDA ejecutar train_all.py para generar los modelos y los archivos

# Paso 1: Obtener Datos de CSV

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math


#9 Dasets acu√≠feros (4 datasets), manantiales (3), un r√≠o y un lago. 
Aquifer_Auser = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Aquifer_Auser.csv")  # pasar la data a un data fram
Aquifer_Doganella = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Aquifer_Doganella.csv")  # pasar la data a un data fram

Aquifer_Luco = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Aquifer_Luco.csv")  # pasar la data a un data fram
Aquifer_Petrignano = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Aquifer_Petrignano.csv")  # pasar la data a un data fram
Lake_Bilancino = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Lake_Bilancino.csv")  # pasar la data a un data fram
River_Arno = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/River_Arno.csv")  # pasar la data a un data fram
Water_Spring_Amiata = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Water_Spring_Amiata.csv")  # pasar la data a un data fram
Water_Spring_Lupa = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Water_Spring_Lupa.csv")  # pasar la data a un data fram
Water_Spring_Madonna_di_Canneto = pd.read_csv("/workspaces/STemporales-Deteccion-Agua-Dan/data/raw/Water_Spring_Madonna_di_Canneto.csv")  # pasar la data a un data fram

#Paso 2 Entender o explorar la data

print("Aquifer_Auser")
print(Aquifer_Auser.head()) #ver rapidamente si cargo la info
print(Aquifer_Auser.columns) # ver las columnas, date object y sales float
print(Aquifer_Auser.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Aquifer_Auser.describe()) # ESTADISTICAS de cada columna,

print("Aquifer_Doganella")
print(Aquifer_Doganella.head()) #ver rapidamente si cargo la info
print(Aquifer_Doganella.columns) # ver las columnas, date object y sales float
print(Aquifer_Doganella.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Aquifer_Doganella.describe()) # ESTADISTICAS de cada columna,

print("Aquifer_Luco")
print(Aquifer_Luco.head()) #ver rapidamente si cargo la info
print(Aquifer_Luco.columns) # ver las columnas, date object y sales float
print(Aquifer_Luco.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Aquifer_Luco.describe()) # ESTADISTICAS de cada columna,

print("Aquifer_Petrignano")
print(Aquifer_Petrignano.head()) #ver rapidamente si cargo la info
print(Aquifer_Petrignano.columns) # ver las columnas, date object y sales float
print(Aquifer_Petrignano.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Aquifer_Petrignano.describe()) # ESTADISTICAS de cada columna,

print("Lake_Bilancino")
print(Lake_Bilancino.head()) #ver rapidamente si cargo la info
print(Lake_Bilancino.columns) # ver las columnas, date object y sales float
print(Lake_Bilancino.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Lake_Bilancino.describe()) # ESTADISTICAS de cada columna,

print("River_Arno")
print(River_Arno.head()) #ver rapidamente si cargo la info
print(River_Arno.columns) # ver las columnas, date object y sales float
print(River_Arno.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(River_Arno.describe()) # ESTADISTICAS de cada columna,

print("Water_Spring_Amiata")
print(Water_Spring_Amiata.head()) #ver rapidamente si cargo la info
print(Water_Spring_Amiata.columns) # ver las columnas, date object y sales float
print(Water_Spring_Amiata.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Water_Spring_Amiata.describe()) # ESTADISTICAS de cada columna,

print("Water_Spring_Lupa")
print(Water_Spring_Lupa.head()) #ver rapidamente si cargo la info
print(Water_Spring_Lupa.columns) # ver las columnas, date object y sales float
print(Water_Spring_Lupa.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Water_Spring_Lupa.describe()) # ESTADISTICAS de cada columna,

print("Water_Spring_Madonna_di_Canneto")
print(Water_Spring_Madonna_di_Canneto.head()) #ver rapidamente si cargo la info
print(Water_Spring_Madonna_di_Canneto.columns) # ver las columnas, date object y sales float
print(Water_Spring_Madonna_di_Canneto.info()) # ves tipos de datos, valores nulos y memoria usada, todo de un vistazo. float64(1), object(1)
print(Water_Spring_Madonna_di_Canneto.describe()) # ESTADISTICAS de cada columna,

"""
Paso 3  Limpieza y Preprocesamiento 

Todos los datasets tienen inconsistencias similares:
* Date est√° como object/string ‚Üí convertir a datetime.
* Much√≠simas columnas con NaN, muchas veces sistem√°ticas (no aleatorias).
* Algunos datasets tienen columnas que son pr√°cticamente constantes ‚Üí eliminarlas.
* Hay valores negativos en variables que no deber√≠an tenerlos (ej. vol√∫menes).
* Fechas no alineadas entre datasets (no se juntan entre s√≠, pero deben ser series continuas internamente).

Por lo que buscaremos crear DF por dataset limpio, indexado por fecha, sin NaN (al menos imputados), 
y con features √∫tiles para modelos de series de tiempo.
"""

# PASO 3A ‚Äî Crear una funci√≥n general de limpieza

def clean_dataset(df):
    import pandas as pd

    # 1. Detectar posibles columnas de fecha
    possible_date_cols = ["cDate", "Date", "date", "Day", "day", "DATE"]

    date_col = None
    for col in df.columns:
        if col in possible_date_cols:
            date_col = col
            break

    # √öltimo recurso: detectar columna con formato de fecha
    if date_col is None:
        for col in df.columns:
            try:
                pd.to_datetime(df[col], dayfirst=True)
                date_col = col
                break
            except:
                pass

    if date_col is None:
        raise ValueError("No se encontr√≥ ninguna columna de fecha en este dataset.")

    # 2. Convertir fechas correctamente
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce", dayfirst=True)

    df = df.dropna(subset=[date_col])
    df = df.sort_values(date_col).set_index(date_col)

    # 3. Convertir objetos a num√©ricos
    df = df.infer_objects(copy=False)
    df = df.apply(pd.to_numeric, errors="coerce")

    # 4. Eliminar columnas completamente vac√≠as
    df = df.dropna(axis=1, how="all")

    # 5. Interpolar usando tiempo
    df = df.interpolate(method="time", limit_direction="both")

    # 6. Rellenar restante sin usar m√©todo deprecated
    df = df.ffill().bfill()

    return df

# PASO 3B ‚Äî Limpiar los 9 datasets

Auser = clean_dataset(Aquifer_Auser)
Doganella = clean_dataset(Aquifer_Doganella)
Luco = clean_dataset(Aquifer_Luco)
Petrignano = clean_dataset(Aquifer_Petrignano)

Bilancino = clean_dataset(Lake_Bilancino)
Arno = clean_dataset(River_Arno)

Amiata = clean_dataset(Water_Spring_Amiata)
Lupa = clean_dataset(Water_Spring_Lupa)
Canneto = clean_dataset(Water_Spring_Madonna_di_Canneto)

# PASO 4 ‚Äî An√°lisis Exploratorio (EDA) para elegir modelos

# 4A ‚Äî Visualizaci√≥n base

def plot_main_variable(df, title):
    plt.figure(figsize=(14,4))
    df.iloc[:,0].plot()
    plt.title(title)
    plt.show()

plot_main_variable(Auser, "Aquifer Auser")
plot_main_variable(Doganella, "Aquifer Doganella")
plot_main_variable(Bilancino, "Lake Bilancino")
plot_main_variable(Arno, "River Arno")


# ---------------------------------------------------
# 1. Detectar columna objetivo autom√°ticamente
# ---------------------------------------------------
def detect_target_column(df):
    num_cols = df.select_dtypes(include=[np.number]).columns

    if len(num_cols) == 1:
        return num_cols[0]

    # Elegir la columna con:
    # - menos NaN
    # - mayor varianza
    # - mayor continuidad temporal
    scores = {}
    for col in num_cols:
        na_score = df[col].isna().mean()
        var_score = df[col].var()
        cont_score = df[col].notna().astype(int).rolling(10).sum().mean()

        scores[col] = (1 - na_score) * 0.4 + (var_score) * 0.4 + (cont_score) * 0.2

    best_col = max(scores, key=scores.get)
    return best_col


# ---------------------------------------------------
# 2. EDA Autom√°tico Completo
# ---------------------------------------------------
def full_eda(df, name="Dataset"):
    print(f"\n======================")
    print(f" EDA: {name}")
    print(f"======================\n")

    # --- Detectar target ---
    target = detect_target_column(df)
    print(f"‚û° Variable objetivo detectada: **{target}**")

    # --- Resumen estad√≠stico ---
    print("\nüìå Resumen estad√≠stico:\n")
    print(df[target].describe())

    # --- Plot 1: Serie completa ---
    plt.figure(figsize=(12,4))
    plt.plot(df[target], linewidth=1)
    plt.title(f"{name} - Serie Temporal ({target})")
    plt.grid(True)
    plt.show()

    # --- Plot 2: Descomposici√≥n estacional ---
    try:
        decomposition = seasonal_decompose(df[target], model="additive", period=365)
        fig = decomposition.plot()
        fig.set_size_inches(12, 8)
        plt.suptitle(f"Descomposici√≥n Estacional - {name}", y=0.93)
        plt.show()
    except:
        print("\n‚ö† No se pudo descomponer (serie muy corta o frecuencia irregular)")

    # --- Plot 3: ACF ---
    plt.figure(figsize=(10,3))
    plot_acf(df[target].dropna(), lags=50)
    plt.title(f"ACF - {name}")
    plt.show()

    # --- Plot 4: PACF ---
    plt.figure(figsize=(10,3))
    plot_pacf(df[target].dropna(), lags=50, method='ywm')
    plt.title(f"PACF - {name}")
    plt.show()

    # --- Detecci√≥n de outliers ---
    q1 = df[target].quantile(0.25)
    q3 = df[target].quantile(0.75)
    iqr = q3 - q1
    outliers = df[(df[target] < q1 - 1.5*iqr) | (df[target] > q3 + 1.5*iqr)]

    print(f"\nüîç Outliers detectados: {len(outliers)}")
    if len(outliers) > 0:
        print(outliers.head())

    # --- Sugerencia de modelo ---
    print("\nü§ñ SUGERENCIA DE MODELO:")

    # Reglas heur√≠sticas basadas en expertos en forecasting
    if df[target].autocorr() > 0.7:
        print("‚Ä¢ Fuerte autocorrelaci√≥n ‚Üí **ARIMA / SARIMA** recomendado")

    if df[target].rolling(365).mean().std() > df[target].std()*0.3:
        print("‚Ä¢ Estacionalidad fuerte ‚Üí **SARIMA o Prophet**")

    if df[target].diff().abs().mean() > df[target].std()*1.2:
        print("‚Ä¢ Mucho ruido ‚Üí **Prophet o RandomForest**")

    if len(df) > 2000:
        print("‚Ä¢ Serie larga ‚Üí **LSTM / Deep Learning** puede ser √∫til")

    print("\n--- EDA COMPLETADO ---\n")




"""

El script train_all.py entren√≥ SARIMA para 9 series y gener√≥ 9 archivos de predicci√≥n.
Los warnings son normales y no afectan el resultado.

* Modelos que salieron bien (buen desempe√±o)
Estos tienen errores bajos ‚Üí modelos estables:

Acu√≠feros

* Auser ‚Üí MAE 0.37, RMSE 0.55
* Petrignano ‚Üí MAE 0.40, RMSE 0.49
* R√≠o Arno ‚Üí MAE 0.49, RMSE 0.77

*  Aceptable / Medio

No perfectos pero razonables:

* Lago Bilancino ‚Üí MAE 1.72, RMSE 3.98
* Manantial Amiata ‚Üí MAE 2.67, RMSE 2.97

* Mal desempe√±o (SARIMA no funciona bien aqu√≠)

SARIMA no est√° capturando la din√°mica, los errores son muy altos:

* Acu√≠feros Doganella ‚Üí MAE 5.19, RMSE 6.74
* Luco ‚Üí MAE 71.27, RMSE 81.44 ‚ùó
* Manantiales Lupa ‚Üí MAE 3.65, RMSE 9.46
* Madonna di Canneto ‚Üí MAE 52.58, RMSE 60.91 ‚ùó

‚Üí Estas series necesitan otro tipo de modelo (XGBoost, LightGBM, LSTM) porque son m√°s complejas y ruidosas.
    """









